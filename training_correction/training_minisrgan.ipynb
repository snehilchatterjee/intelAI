{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import models\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2ycbcr\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(Dataset):\n",
    "    def __init__(self, LR_path, GT_path, in_memory = True, transform = None):\n",
    "        \n",
    "        self.LR_path = LR_path\n",
    "        self.GT_path = GT_path\n",
    "        self.in_memory = in_memory\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.LR_img = sorted(os.listdir(LR_path))\n",
    "        self.GT_img = sorted(os.listdir(GT_path))\n",
    "        \n",
    "        if in_memory:\n",
    "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr)).convert(\"RGB\")).astype(np.uint8) for lr in self.LR_img]\n",
    "            self.GT_img = [np.array(Image.open(os.path.join(self.GT_path, gt)).convert(\"RGB\")).astype(np.uint8) for gt in self.GT_img]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.LR_img)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img_item = {}\n",
    "        \n",
    "        if self.in_memory:\n",
    "            GT = self.GT_img[i].astype(np.float32)\n",
    "            LR = self.LR_img[i].astype(np.float32)\n",
    "            \n",
    "        else:\n",
    "            GT = np.array(Image.open(os.path.join(self.GT_path, self.GT_img[i])).convert(\"RGB\"))\n",
    "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])).convert(\"RGB\"))\n",
    "\n",
    "        img_item['GT'] = (GT / 127.5) - 1.0\n",
    "        img_item['LR'] = (LR / 127.5) - 1.0\n",
    "                \n",
    "        if self.transform is not None:\n",
    "            img_item = self.transform(img_item)\n",
    "            \n",
    "        img_item['GT'] = img_item['GT'].transpose(2, 0, 1).astype(np.float32)\n",
    "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        return img_item\n",
    "    \n",
    "    \n",
    "class testOnly_data(Dataset):\n",
    "    def __init__(self, LR_path, in_memory = True, transform = None):\n",
    "        \n",
    "        self.LR_path = LR_path\n",
    "        self.LR_img = sorted(os.listdir(LR_path))\n",
    "        self.in_memory = in_memory\n",
    "        if in_memory:\n",
    "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr))) for lr in self.LR_img]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.LR_img)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img_item = {}\n",
    "        \n",
    "        if self.in_memory:\n",
    "            LR = self.LR_img[i]\n",
    "            \n",
    "        else:\n",
    "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])))\n",
    "\n",
    "        img_item['LR'] = (LR / 127.5) - 1.0                \n",
    "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        return img_item\n",
    "\n",
    "\n",
    "class crop(object):\n",
    "    def __init__(self, scale, patch_size):\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        LR_img, GT_img = sample['LR'], sample['GT']\n",
    "        ih, iw = LR_img.shape[:2]\n",
    "        \n",
    "        ix = random.randrange(0, iw - self.patch_size +1)\n",
    "        iy = random.randrange(0, ih - self.patch_size +1)\n",
    "        \n",
    "        tx = ix * self.scale\n",
    "        ty = iy * self.scale\n",
    "        \n",
    "        LR_patch = LR_img[iy : iy + self.patch_size, ix : ix + self.patch_size]\n",
    "        GT_patch = GT_img[ty : ty + (self.scale * self.patch_size), tx : tx + (self.scale * self.patch_size)]\n",
    "        \n",
    "        return {'LR' : LR_patch, 'GT' : GT_patch}\n",
    "\n",
    "class augmentation(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        LR_img, GT_img = sample['LR'], sample['GT']\n",
    "        \n",
    "        hor_flip = random.randrange(0,2)\n",
    "        ver_flip = random.randrange(0,2)\n",
    "        rot = random.randrange(0,2)\n",
    "    \n",
    "        if hor_flip:\n",
    "            temp_LR = np.fliplr(LR_img)\n",
    "            LR_img = temp_LR.copy()\n",
    "            temp_GT = np.fliplr(GT_img)\n",
    "            GT_img = temp_GT.copy()\n",
    "            \n",
    "            del temp_LR, temp_GT\n",
    "        \n",
    "        if ver_flip:\n",
    "            temp_LR = np.flipud(LR_img)\n",
    "            LR_img = temp_LR.copy()\n",
    "            temp_GT = np.flipud(GT_img)\n",
    "            GT_img = temp_GT.copy()\n",
    "            \n",
    "            del temp_LR, temp_GT\n",
    "            \n",
    "        if rot:\n",
    "            LR_img = LR_img.transpose(1, 0, 2)\n",
    "            GT_img = GT_img.transpose(1, 0, 2)\n",
    "        \n",
    "        \n",
    "        return {'LR' : LR_img, 'GT' : GT_img}\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 for Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg19(nn.Module):\n",
    "    \n",
    "    def __init__(self, pre_trained = True, require_grad = False):\n",
    "        super(vgg19, self).__init__()\n",
    "        self.vgg_feature = models.vgg19(pretrained = pre_trained).features\n",
    "        self.seq_list = [nn.Sequential(ele) for ele in self.vgg_feature]\n",
    "        self.vgg_layer = ['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', \n",
    "                         'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "                         'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "                         'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "                         'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5']\n",
    "        \n",
    "        if not require_grad:\n",
    "            for parameter in self.parameters():\n",
    "                parameter.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv1_1 = self.seq_list[0](x)\n",
    "        relu1_1 = self.seq_list[1](conv1_1)\n",
    "        conv1_2 = self.seq_list[2](relu1_1)\n",
    "        relu1_2 = self.seq_list[3](conv1_2)\n",
    "        pool1 = self.seq_list[4](relu1_2)\n",
    "        \n",
    "        conv2_1 = self.seq_list[5](pool1)\n",
    "        relu2_1 = self.seq_list[6](conv2_1)\n",
    "        conv2_2 = self.seq_list[7](relu2_1)\n",
    "        relu2_2 = self.seq_list[8](conv2_2)\n",
    "        pool2 = self.seq_list[9](relu2_2)\n",
    "        \n",
    "        conv3_1 = self.seq_list[10](pool2)\n",
    "        relu3_1 = self.seq_list[11](conv3_1)\n",
    "        conv3_2 = self.seq_list[12](relu3_1)\n",
    "        relu3_2 = self.seq_list[13](conv3_2)\n",
    "        conv3_3 = self.seq_list[14](relu3_2)\n",
    "        relu3_3 = self.seq_list[15](conv3_3)\n",
    "        conv3_4 = self.seq_list[16](relu3_3)\n",
    "        relu3_4 = self.seq_list[17](conv3_4)\n",
    "        pool3 = self.seq_list[18](relu3_4)\n",
    "        \n",
    "        conv4_1 = self.seq_list[19](pool3)\n",
    "        relu4_1 = self.seq_list[20](conv4_1)\n",
    "        conv4_2 = self.seq_list[21](relu4_1)\n",
    "        relu4_2 = self.seq_list[22](conv4_2)\n",
    "        conv4_3 = self.seq_list[23](relu4_2)\n",
    "        relu4_3 = self.seq_list[24](conv4_3)\n",
    "        conv4_4 = self.seq_list[25](relu4_3)\n",
    "        relu4_4 = self.seq_list[26](conv4_4)\n",
    "        pool4 = self.seq_list[27](relu4_4)\n",
    "        \n",
    "        conv5_1 = self.seq_list[28](pool4)\n",
    "        relu5_1 = self.seq_list[29](conv5_1)\n",
    "        conv5_2 = self.seq_list[30](relu5_1)\n",
    "        relu5_2 = self.seq_list[31](conv5_2)\n",
    "        conv5_3 = self.seq_list[32](relu5_2)\n",
    "        relu5_3 = self.seq_list[33](conv5_3)\n",
    "        conv5_4 = self.seq_list[34](relu5_3)\n",
    "        relu5_4 = self.seq_list[35](conv5_4)\n",
    "        pool5 = self.seq_list[36](relu5_4)\n",
    "        \n",
    "        vgg_output = namedtuple(\"vgg_output\", self.vgg_layer)\n",
    "        \n",
    "        vgg_list = [conv1_1, relu1_1, conv1_2, relu1_2, pool1, \n",
    "                         conv2_1, relu2_1, conv2_2, relu2_2, pool2,\n",
    "                         conv3_1, relu3_1, conv3_2, relu3_2, conv3_3, relu3_3, conv3_4, relu3_4, pool3,\n",
    "                         conv4_1, relu4_1, conv4_2, relu4_2, conv4_3, relu4_3, conv4_4, relu4_4, pool4,\n",
    "                         conv5_1, relu5_1, conv5_2, relu5_2, conv5_3, relu5_3, conv5_4, relu5_4, pool5]\n",
    "        \n",
    "        out = vgg_output(*vgg_list)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, rgb_range = 1,\n",
    "        norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), sign=-1):\n",
    "\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(norm_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(norm_mean) / std\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "class perceptual_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, vgg):\n",
    "        super(perceptual_loss, self).__init__()\n",
    "        self.normalization_mean = [0.485, 0.456, 0.406]\n",
    "        self.normalization_std = [0.229, 0.224, 0.225]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.transform = MeanShift(norm_mean = self.normalization_mean, norm_std = self.normalization_std).to(self.device)\n",
    "        self.vgg = vgg\n",
    "        self.criterion = nn.MSELoss()\n",
    "    def forward(self, HR, SR, layer = 'relu5_4'):\n",
    "        ## HR and SR should be normalized [0,1]\n",
    "        hr = self.transform(HR)\n",
    "        sr = self.transform(SR)\n",
    "        \n",
    "        hr_feat = getattr(self.vgg(hr), layer)\n",
    "        sr_feat = getattr(self.vgg(sr), layer)\n",
    "        \n",
    "        return self.criterion(hr_feat, sr_feat), hr_feat, sr_feat\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        \n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "        super(_conv, self).__init__(in_channels = in_channels, out_channels = out_channels, \n",
    "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True)\n",
    "        \n",
    "        self.weight.data = torch.normal(torch.zeros((out_channels, in_channels, kernel_size, kernel_size)), 0.02)\n",
    "        self.bias.data = torch.zeros((out_channels))\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, BN = False, act = None, stride = 1, bias = True):\n",
    "        super(conv, self).__init__()\n",
    "        m = []\n",
    "        m.append(_conv(in_channels = in_channel, out_channels = out_channel, \n",
    "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True))\n",
    "        \n",
    "        if BN:\n",
    "            m.append(nn.BatchNorm2d(num_features = out_channel))\n",
    "        \n",
    "        if act is not None:\n",
    "            m.append(act)\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, act = nn.ReLU(inplace = True), bias = True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(channels, channels, kernel_size, BN = True, act = act))\n",
    "        m.append(conv(channels, channels, kernel_size, BN = True, act = None))\n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, num_res_block, act = nn.ReLU(inplace = True)):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        m = []\n",
    "        \n",
    "        self.conv = conv(in_channels, out_channels, kernel_size, BN = False, act = act)\n",
    "        for i in range(num_res_block):\n",
    "            m.append(ResBlock(out_channels, kernel_size, act))\n",
    "        \n",
    "        m.append(conv(out_channels, out_channels, kernel_size, BN = True, act = None))\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.conv(x)\n",
    "        out = self.body(res)\n",
    "        out += res\n",
    "        \n",
    "        return out\n",
    "        \n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, channel, kernel_size, scale, act = nn.ReLU(inplace = True)):\n",
    "        super(Upsampler, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(channel, channel * scale * scale, kernel_size))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "    \n",
    "        if act is not None:\n",
    "            m.append(act)\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n",
    "\n",
    "class discrim_block(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, kernel_size, act = nn.LeakyReLU(inplace = True)):\n",
    "        super(discrim_block, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(in_feats, out_feats, kernel_size, BN = True, act = act))\n",
    "        m.append(conv(out_feats, out_feats, kernel_size, BN = True, act = act, stride = 2))\n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniSRGAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_feat = 3, n_feats = 32, kernel_size = 3, num_block = 6, act = nn.PReLU(), scale=4):\n",
    "        super(MiniSRGAN, self).__init__()\n",
    "        \n",
    "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 9, BN = False, act = act)\n",
    "        \n",
    "        resblocks = [ResBlock(channels = n_feats, kernel_size = 3, act = act) for _ in range(num_block)]\n",
    "        self.body = nn.Sequential(*resblocks)\n",
    "        \n",
    "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = True, act = None)\n",
    "        \n",
    "        if(scale == 4):\n",
    "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = 2, act = act) for _ in range(2)]\n",
    "        else:\n",
    "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = scale, act = act)]\n",
    "\n",
    "        self.tail = nn.Sequential(*upsample_blocks)\n",
    "        \n",
    "        self.last_conv = conv(in_channel = n_feats, out_channel = img_feat, kernel_size = 3, BN = False, act = nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv01(x)\n",
    "        _skip_connection = x\n",
    "        \n",
    "        x = self.body(x)\n",
    "        x = self.conv02(x)\n",
    "        feat = x + _skip_connection\n",
    "        \n",
    "        x = self.tail(feat)\n",
    "        x = self.last_conv(x)\n",
    "        \n",
    "        return x, feat\n",
    "     \n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_feat = 3, n_feats = 64, kernel_size = 3, act = nn.LeakyReLU(inplace = True), num_of_block = 3, patch_size = 96):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.act = act\n",
    "        \n",
    "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act)\n",
    "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act, stride = 2)\n",
    "        \n",
    "        body = [discrim_block(in_feats = n_feats * (2 ** i), out_feats = n_feats * (2 ** (i + 1)), kernel_size = 3, act = self.act) for i in range(num_of_block)]    \n",
    "        self.body = nn.Sequential(*body)\n",
    "        \n",
    "        self.linear_size = ((patch_size // (2 ** (num_of_block + 1))) ** 2) * (n_feats * (2 ** num_of_block))\n",
    "        \n",
    "        tail = []\n",
    "        \n",
    "        tail.append(nn.Linear(self.linear_size, 1024))\n",
    "        tail.append(self.act)\n",
    "        tail.append(nn.Linear(1024, 1))\n",
    "        tail.append(nn.Sigmoid())\n",
    "        \n",
    "        self.tail = nn.Sequential(*tail)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv01(x)\n",
    "        x = self.conv02(x)\n",
    "        x = self.body(x)        \n",
    "        x = x.view(-1, self.linear_size)\n",
    "        x = self.tail(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(LR_path, GT_path, batch_size=16, res_num=16, num_workers=0, scale=4, L2_coeff=1.0, adv_coeff=1e-3, tv_loss_coeff=0.0, pre_train_epoch=8000, fine_train_epoch=4000, patch_size=24, feat_layer='relu5_4', vgg_rescale_coeff=0.006, in_memory=True, generator_path=None, fine_tuning=False):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    transform  = transforms.Compose([crop(scale, patch_size), augmentation()])\n",
    "    dataset = mydata(GT_path=GT_path, LR_path=LR_path, in_memory=in_memory, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    generator = MiniSRGAN()\n",
    "    \n",
    "    if fine_tuning:        \n",
    "        generator.load_state_dict(torch.load(generator_path))\n",
    "        print(\"pre-trained model is loaded\")\n",
    "        print(\"path : %s\"%(generator_path))\n",
    "        \n",
    "    generator = generator.to(device)\n",
    "    generator.train()\n",
    "    \n",
    "    l2_loss = nn.MSELoss()\n",
    "    g_optim = optim.Adam(generator.parameters(), lr=1e-4)\n",
    "        \n",
    "    pre_epoch = 0\n",
    "    fine_epoch = 0\n",
    "    \n",
    "    #### Train using L2_loss\n",
    "    # Initialize best loss to a very large value\n",
    "    best_loss = 0.002\n",
    "    pre_epoch = 0\n",
    "\n",
    "    while pre_epoch < pre_train_epoch:\n",
    "        for i, tr_data in enumerate(tqdm(loader, desc=f\"Epoch {pre_epoch+1}/{pre_train_epoch}\")):\n",
    "            gt = tr_data['GT'].to(device)\n",
    "            lr = tr_data['LR'].to(device)\n",
    "\n",
    "            output, _ = generator(lr)\n",
    "            loss = l2_loss(gt, output)\n",
    "\n",
    "            g_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            g_optim.step()\n",
    "\n",
    "        pre_epoch += 1\n",
    "\n",
    "        if pre_epoch % 2 == 0:\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/pre_trained_model_latest.pt')\n",
    "            print(pre_epoch)\n",
    "            print(loss.item())\n",
    "            print('=========')\n",
    "\n",
    "        # Save the model if it has the best loss so far\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/best_pre_trained_model.pt')\n",
    "            print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "        if pre_epoch % 800 == 0:\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/pre_trained_model_%03d.pt' % pre_epoch)\n",
    "    \n",
    "    #### Train using perceptual & adversarial loss\n",
    "    vgg_net = vgg19().to(device)\n",
    "    vgg_net = vgg_net.eval()\n",
    "    \n",
    "    discriminator = Discriminator(patch_size=patch_size * scale)\n",
    "    discriminator = discriminator.to(device)\n",
    "    discriminator.train()\n",
    "    \n",
    "    d_optim = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(g_optim, step_size=2000, gamma=0.1)\n",
    "    \n",
    "    VGG_loss = perceptual_loss(vgg_net)\n",
    "    cross_ent = nn.BCELoss()\n",
    "    tv_loss = TVLoss()\n",
    "    real_label = torch.ones((batch_size, 1)).to(device)\n",
    "    fake_label = torch.zeros((batch_size, 1)).to(device)\n",
    "    \n",
    "    generator.load_state_dict(torch.load('./minisrgan_weights/pre_trained_model_latest.pt'))\n",
    "\n",
    "    print('Training Discriminator and Generator...')\n",
    "    while fine_epoch < fine_train_epoch:\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Initialize tqdm progress bar\n",
    "        for i, tr_data in enumerate(tqdm(loader, desc=f\"Epoch {fine_epoch+1}/{fine_train_epoch}\")):\n",
    "            gt = tr_data['GT'].to(device)\n",
    "            lr = tr_data['LR'].to(device)\n",
    "\n",
    "                            \n",
    "            ## Training Discriminator\n",
    "            output, _ = generator(lr)\n",
    "            fake_prob = discriminator(output)\n",
    "            real_prob = discriminator(gt)\n",
    "            \n",
    "            d_loss_real = cross_ent(real_prob, real_label)\n",
    "            d_loss_fake = cross_ent(fake_prob, fake_label)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            g_optim.zero_grad()\n",
    "            d_optim.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "            \n",
    "            ## Training Generator\n",
    "            output, _ = generator(lr)\n",
    "            fake_prob = discriminator(output)\n",
    "            \n",
    "            _percep_loss, hr_feat, sr_feat = VGG_loss((gt + 1.0) / 2.0, (output + 1.0) / 2.0, layer=feat_layer)\n",
    "            \n",
    "            L2_loss = l2_loss(output, gt)\n",
    "            percep_loss = vgg_rescale_coeff * _percep_loss\n",
    "            adversarial_loss = adv_coeff * cross_ent(fake_prob, real_label)\n",
    "            total_variance_loss = tv_loss_coeff * tv_loss(vgg_rescale_coeff * (hr_feat - sr_feat)**2)\n",
    "            \n",
    "            g_loss = percep_loss + adversarial_loss + total_variance_loss + L2_loss\n",
    "            \n",
    "            if g_loss.item() < best_loss:\n",
    "                best_loss = g_loss.item()\n",
    "                torch.save(generator.state_dict(), './minisrgan_weights/best_trained_model.pt')\n",
    "                print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "            \n",
    "            g_optim.zero_grad()\n",
    "            d_optim.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optim.step()\n",
    "\n",
    "            \n",
    "        fine_epoch += 1\n",
    "\n",
    "        if fine_epoch % 2 == 0:\n",
    "            print(fine_epoch)\n",
    "            print(g_loss.item())\n",
    "            print(d_loss.item())\n",
    "            print('=========')\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/latest_trained_model.pt')\n",
    "\n",
    "        if fine_epoch % 500 == 0:\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/SRGAN_gene_%03d.pt' % fine_epoch)\n",
    "            torch.save(discriminator.state_dict(), './minisrgan_weights/SRGAN_discrim_%03d.pt' % fine_epoch)\n",
    "\n",
    "\n",
    "def test(LR_path, GT_path, num_workers=0, scale=4, generator_path=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = mydata(GT_path=GT_path, LR_path=LR_path, in_memory=False, transform=None)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    generator = MiniSRGAN()\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    generator = generator.to(device)\n",
    "    generator.eval()\n",
    "    \n",
    "    f = open('./result_minisrgan.txt', 'w')\n",
    "    psnr_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, te_data in enumerate(loader):\n",
    "            gt = te_data['GT'].to(device)\n",
    "            lr = te_data['LR'].to(device)\n",
    "\n",
    "            bs, c, h, w = lr.size()\n",
    "            gt = gt[:, :, : h * scale, : w * scale]\n",
    "\n",
    "            output, _ = generator(lr)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "            output = np.clip(output, -1.0, 1.0)\n",
    "            gt = gt[0].cpu().numpy()\n",
    "\n",
    "            output = (output + 1.0) / 2.0\n",
    "            gt = (gt + 1.0) / 2.0\n",
    "\n",
    "            output = output.transpose(1, 2, 0)\n",
    "            gt = gt.transpose(1, 2, 0)\n",
    "\n",
    "            y_output = rgb2ycbcr(output)[scale:-scale, scale:-scale, :1]\n",
    "            y_gt = rgb2ycbcr(gt)[scale:-scale, scale:-scale, :1]\n",
    "            \n",
    "            psnr_value = peak_signal_noise_ratio(y_output / 255.0, y_gt / 255.0, data_range=1.0)\n",
    "            psnr_list.append(psnr_value)\n",
    "            f.write(f'PSNR: {psnr_value:.4f}\\n')\n",
    "\n",
    "            result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
    "            result.save('./result_minisrgan/res_%04d.png' % i)\n",
    "\n",
    "        f.write(f'Average PSNR: {np.mean(psnr_list):.4f}')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def test_only(LR_path, num_workers=0, generator_path=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = testOnly_data(LR_path=LR_path, in_memory=False, transform=None)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    generator = MiniSRGAN()\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    generator = generator.to(device)\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, te_data in enumerate(loader):\n",
    "            lr = te_data['LR'].to(device)\n",
    "            output, _ = generator(lr)\n",
    "            output = output[0].cpu().numpy()\n",
    "            output = (output + 1.0) / 2.0\n",
    "            output = output.transpose(1, 2, 0)\n",
    "            result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
    "            result.save('./result_minisrgan/res_%04d.png' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(GT_path='/media/moose/Main\\ Volume/DIV2K_Complete/DIV2K_train',LR_path='/media/moose/Main\\ Volume/DIV2K_Complete/DIV2K_train_LR_bicubic/X4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './result_mobilesr/res_0000.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGT_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../../../Set5/image_SRF_4/HR/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../../../Set5/image_SRF_4/LR/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./minisrgan_weights/pre_trained_model_latest.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 185\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(LR_path, GT_path, num_workers, scale, generator_path)\u001b[0m\n\u001b[1;32m    182\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPSNR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpsnr_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    184\u001b[0m     result \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray((output \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./result_mobilesr/res_\u001b[39;49m\u001b[38;5;132;43;01m%04d\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage PSNR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(psnr_list)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    188\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/PIL/Image.py:2410\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2408\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2410\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2413\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './result_mobilesr/res_0000.png'"
     ]
    }
   ],
   "source": [
    "test(GT_path='../../../../Set5/image_SRF_4/HR/', LR_path='../../../../Set5/image_SRF_4/LR/', generator_path='./minisrgan_weights/pre_trained_model_latest.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
