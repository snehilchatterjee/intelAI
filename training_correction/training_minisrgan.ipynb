{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import models\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2ycbcr\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(Dataset):\n",
    "    def __init__(self, LR_path, GT_path, in_memory = True, transform = None):\n",
    "        \n",
    "        self.LR_path = LR_path\n",
    "        self.GT_path = GT_path\n",
    "        self.in_memory = in_memory\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.LR_img = sorted(os.listdir(LR_path))\n",
    "        self.GT_img = sorted(os.listdir(GT_path))\n",
    "        \n",
    "        if in_memory:\n",
    "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr)).convert(\"RGB\")).astype(np.uint8) for lr in self.LR_img]\n",
    "            self.GT_img = [np.array(Image.open(os.path.join(self.GT_path, gt)).convert(\"RGB\")).astype(np.uint8) for gt in self.GT_img]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.LR_img)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img_item = {}\n",
    "        \n",
    "        if self.in_memory:\n",
    "            GT = self.GT_img[i].astype(np.float32)\n",
    "            LR = self.LR_img[i].astype(np.float32)\n",
    "            \n",
    "        else:\n",
    "            GT = np.array(Image.open(os.path.join(self.GT_path, self.GT_img[i])).convert(\"RGB\"))\n",
    "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])).convert(\"RGB\"))\n",
    "\n",
    "        img_item['GT'] = (GT / 127.5) - 1.0\n",
    "        img_item['LR'] = (LR / 127.5) - 1.0\n",
    "                \n",
    "        if self.transform is not None:\n",
    "            img_item = self.transform(img_item)\n",
    "            \n",
    "        img_item['GT'] = img_item['GT'].transpose(2, 0, 1).astype(np.float32)\n",
    "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        return img_item\n",
    "    \n",
    "    \n",
    "class testOnly_data(Dataset):\n",
    "    def __init__(self, LR_path, in_memory = True, transform = None):\n",
    "        \n",
    "        self.LR_path = LR_path\n",
    "        self.LR_img = sorted(os.listdir(LR_path))\n",
    "        self.in_memory = in_memory\n",
    "        if in_memory:\n",
    "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr))) for lr in self.LR_img]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.LR_img)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img_item = {}\n",
    "        \n",
    "        if self.in_memory:\n",
    "            LR = self.LR_img[i]\n",
    "            \n",
    "        else:\n",
    "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])))\n",
    "\n",
    "        img_item['LR'] = (LR / 127.5) - 1.0                \n",
    "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        return img_item\n",
    "\n",
    "\n",
    "class crop(object):\n",
    "    def __init__(self, scale, patch_size):\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        LR_img, GT_img = sample['LR'], sample['GT']\n",
    "        ih, iw = LR_img.shape[:2]\n",
    "        \n",
    "        ix = random.randrange(0, iw - self.patch_size +1)\n",
    "        iy = random.randrange(0, ih - self.patch_size +1)\n",
    "        \n",
    "        tx = ix * self.scale\n",
    "        ty = iy * self.scale\n",
    "        \n",
    "        LR_patch = LR_img[iy : iy + self.patch_size, ix : ix + self.patch_size]\n",
    "        GT_patch = GT_img[ty : ty + (self.scale * self.patch_size), tx : tx + (self.scale * self.patch_size)]\n",
    "        \n",
    "        return {'LR' : LR_patch, 'GT' : GT_patch}\n",
    "\n",
    "class augmentation(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        LR_img, GT_img = sample['LR'], sample['GT']\n",
    "        \n",
    "        hor_flip = random.randrange(0,2)\n",
    "        ver_flip = random.randrange(0,2)\n",
    "        rot = random.randrange(0,2)\n",
    "    \n",
    "        if hor_flip:\n",
    "            temp_LR = np.fliplr(LR_img)\n",
    "            LR_img = temp_LR.copy()\n",
    "            temp_GT = np.fliplr(GT_img)\n",
    "            GT_img = temp_GT.copy()\n",
    "            \n",
    "            del temp_LR, temp_GT\n",
    "        \n",
    "        if ver_flip:\n",
    "            temp_LR = np.flipud(LR_img)\n",
    "            LR_img = temp_LR.copy()\n",
    "            temp_GT = np.flipud(GT_img)\n",
    "            GT_img = temp_GT.copy()\n",
    "            \n",
    "            del temp_LR, temp_GT\n",
    "            \n",
    "        if rot:\n",
    "            LR_img = LR_img.transpose(1, 0, 2)\n",
    "            GT_img = GT_img.transpose(1, 0, 2)\n",
    "        \n",
    "        \n",
    "        return {'LR' : LR_img, 'GT' : GT_img}\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 for Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg19(nn.Module):\n",
    "    \n",
    "    def __init__(self, pre_trained = True, require_grad = False):\n",
    "        super(vgg19, self).__init__()\n",
    "        self.vgg_feature = models.vgg19(pretrained = pre_trained).features\n",
    "        self.seq_list = [nn.Sequential(ele) for ele in self.vgg_feature]\n",
    "        self.vgg_layer = ['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', \n",
    "                         'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "                         'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "                         'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "                         'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5']\n",
    "        \n",
    "        if not require_grad:\n",
    "            for parameter in self.parameters():\n",
    "                parameter.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv1_1 = self.seq_list[0](x)\n",
    "        relu1_1 = self.seq_list[1](conv1_1)\n",
    "        conv1_2 = self.seq_list[2](relu1_1)\n",
    "        relu1_2 = self.seq_list[3](conv1_2)\n",
    "        pool1 = self.seq_list[4](relu1_2)\n",
    "        \n",
    "        conv2_1 = self.seq_list[5](pool1)\n",
    "        relu2_1 = self.seq_list[6](conv2_1)\n",
    "        conv2_2 = self.seq_list[7](relu2_1)\n",
    "        relu2_2 = self.seq_list[8](conv2_2)\n",
    "        pool2 = self.seq_list[9](relu2_2)\n",
    "        \n",
    "        conv3_1 = self.seq_list[10](pool2)\n",
    "        relu3_1 = self.seq_list[11](conv3_1)\n",
    "        conv3_2 = self.seq_list[12](relu3_1)\n",
    "        relu3_2 = self.seq_list[13](conv3_2)\n",
    "        conv3_3 = self.seq_list[14](relu3_2)\n",
    "        relu3_3 = self.seq_list[15](conv3_3)\n",
    "        conv3_4 = self.seq_list[16](relu3_3)\n",
    "        relu3_4 = self.seq_list[17](conv3_4)\n",
    "        pool3 = self.seq_list[18](relu3_4)\n",
    "        \n",
    "        conv4_1 = self.seq_list[19](pool3)\n",
    "        relu4_1 = self.seq_list[20](conv4_1)\n",
    "        conv4_2 = self.seq_list[21](relu4_1)\n",
    "        relu4_2 = self.seq_list[22](conv4_2)\n",
    "        conv4_3 = self.seq_list[23](relu4_2)\n",
    "        relu4_3 = self.seq_list[24](conv4_3)\n",
    "        conv4_4 = self.seq_list[25](relu4_3)\n",
    "        relu4_4 = self.seq_list[26](conv4_4)\n",
    "        pool4 = self.seq_list[27](relu4_4)\n",
    "        \n",
    "        conv5_1 = self.seq_list[28](pool4)\n",
    "        relu5_1 = self.seq_list[29](conv5_1)\n",
    "        conv5_2 = self.seq_list[30](relu5_1)\n",
    "        relu5_2 = self.seq_list[31](conv5_2)\n",
    "        conv5_3 = self.seq_list[32](relu5_2)\n",
    "        relu5_3 = self.seq_list[33](conv5_3)\n",
    "        conv5_4 = self.seq_list[34](relu5_3)\n",
    "        relu5_4 = self.seq_list[35](conv5_4)\n",
    "        pool5 = self.seq_list[36](relu5_4)\n",
    "        \n",
    "        vgg_output = namedtuple(\"vgg_output\", self.vgg_layer)\n",
    "        \n",
    "        vgg_list = [conv1_1, relu1_1, conv1_2, relu1_2, pool1, \n",
    "                         conv2_1, relu2_1, conv2_2, relu2_2, pool2,\n",
    "                         conv3_1, relu3_1, conv3_2, relu3_2, conv3_3, relu3_3, conv3_4, relu3_4, pool3,\n",
    "                         conv4_1, relu4_1, conv4_2, relu4_2, conv4_3, relu4_3, conv4_4, relu4_4, pool4,\n",
    "                         conv5_1, relu5_1, conv5_2, relu5_2, conv5_3, relu5_3, conv5_4, relu5_4, pool5]\n",
    "        \n",
    "        out = vgg_output(*vgg_list)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, rgb_range = 1,\n",
    "        norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), sign=-1):\n",
    "\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(norm_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(norm_mean) / std\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "class perceptual_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, vgg):\n",
    "        super(perceptual_loss, self).__init__()\n",
    "        self.normalization_mean = [0.485, 0.456, 0.406]\n",
    "        self.normalization_std = [0.229, 0.224, 0.225]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.transform = MeanShift(norm_mean = self.normalization_mean, norm_std = self.normalization_std).to(self.device)\n",
    "        self.vgg = vgg\n",
    "        self.criterion = nn.MSELoss()\n",
    "    def forward(self, HR, SR, layer = 'relu5_4'):\n",
    "        ## HR and SR should be normalized [0,1]\n",
    "        hr = self.transform(HR)\n",
    "        sr = self.transform(SR)\n",
    "        \n",
    "        hr_feat = getattr(self.vgg(hr), layer)\n",
    "        sr_feat = getattr(self.vgg(sr), layer)\n",
    "        \n",
    "        return self.criterion(hr_feat, sr_feat), hr_feat, sr_feat\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        \n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "        super(_conv, self).__init__(in_channels = in_channels, out_channels = out_channels, \n",
    "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True)\n",
    "        \n",
    "        self.weight.data = torch.normal(torch.zeros((out_channels, in_channels, kernel_size, kernel_size)), 0.02)\n",
    "        self.bias.data = torch.zeros((out_channels))\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, BN = False, act = None, stride = 1, bias = True):\n",
    "        super(conv, self).__init__()\n",
    "        m = []\n",
    "        m.append(_conv(in_channels = in_channel, out_channels = out_channel, \n",
    "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True))\n",
    "        \n",
    "        if BN:\n",
    "            m.append(nn.BatchNorm2d(num_features = out_channel))\n",
    "        \n",
    "        if act is not None:\n",
    "            m.append(act)\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, act = nn.ReLU(inplace = True), bias = True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(channels, channels, kernel_size, BN = True, act = act))\n",
    "        m.append(conv(channels, channels, kernel_size, BN = True, act = None))\n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, num_res_block, act = nn.ReLU(inplace = True)):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        m = []\n",
    "        \n",
    "        self.conv = conv(in_channels, out_channels, kernel_size, BN = False, act = act)\n",
    "        for i in range(num_res_block):\n",
    "            m.append(ResBlock(out_channels, kernel_size, act))\n",
    "        \n",
    "        m.append(conv(out_channels, out_channels, kernel_size, BN = True, act = None))\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.conv(x)\n",
    "        out = self.body(res)\n",
    "        out += res\n",
    "        \n",
    "        return out\n",
    "        \n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, channel, kernel_size, scale, act = nn.ReLU(inplace = True)):\n",
    "        super(Upsampler, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(channel, channel * scale * scale, kernel_size))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "    \n",
    "        if act is not None:\n",
    "            m.append(act)\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n",
    "\n",
    "class discrim_block(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, kernel_size, act = nn.LeakyReLU(inplace = True)):\n",
    "        super(discrim_block, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(in_feats, out_feats, kernel_size, BN = True, act = act))\n",
    "        m.append(conv(out_feats, out_feats, kernel_size, BN = True, act = act, stride = 2))\n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniSRGAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_feat = 3, n_feats = 64, kernel_size = 3, num_block = 8, act = nn.PReLU(), scale=4):\n",
    "        super(MiniSRGAN, self).__init__()\n",
    "        \n",
    "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 9, BN = False, act = act)\n",
    "        \n",
    "        resblocks = [ResBlock(channels = n_feats, kernel_size = 3, act = act) for _ in range(num_block)]\n",
    "        self.body = nn.Sequential(*resblocks)\n",
    "        \n",
    "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = True, act = None)\n",
    "        \n",
    "        if(scale == 4):\n",
    "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = 2, act = act) for _ in range(2)]\n",
    "        else:\n",
    "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = scale, act = act)]\n",
    "\n",
    "        self.tail = nn.Sequential(*upsample_blocks)\n",
    "        \n",
    "        self.last_conv = conv(in_channel = n_feats, out_channel = img_feat, kernel_size = 3, BN = False, act = nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv01(x)\n",
    "        _skip_connection = x\n",
    "        \n",
    "        x = self.body(x)\n",
    "        x = self.conv02(x)\n",
    "        feat = x + _skip_connection\n",
    "        \n",
    "        x = self.tail(feat)\n",
    "        x = self.last_conv(x)\n",
    "        \n",
    "        return x, feat\n",
    "     \n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_feat = 3, n_feats = 64, kernel_size = 3, act = nn.LeakyReLU(inplace = True), num_of_block = 3, patch_size = 96):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.act = act\n",
    "        \n",
    "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act)\n",
    "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act, stride = 2)\n",
    "        \n",
    "        body = [discrim_block(in_feats = n_feats * (2 ** i), out_feats = n_feats * (2 ** (i + 1)), kernel_size = 3, act = self.act) for i in range(num_of_block)]    \n",
    "        self.body = nn.Sequential(*body)\n",
    "        \n",
    "        self.linear_size = ((patch_size // (2 ** (num_of_block + 1))) ** 2) * (n_feats * (2 ** num_of_block))\n",
    "        \n",
    "        tail = []\n",
    "        \n",
    "        tail.append(nn.Linear(self.linear_size, 1024))\n",
    "        tail.append(self.act)\n",
    "        tail.append(nn.Linear(1024, 1))\n",
    "        tail.append(nn.Sigmoid())\n",
    "        \n",
    "        self.tail = nn.Sequential(*tail)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv01(x)\n",
    "        x = self.conv02(x)\n",
    "        x = self.body(x)        \n",
    "        x = x.view(-1, self.linear_size)\n",
    "        x = self.tail(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(LR_path, GT_path, batch_size=16, res_num=16, num_workers=0, scale=4, L2_coeff=1.0, adv_coeff=1e-3, tv_loss_coeff=0.0, pre_train_epoch=8000, fine_train_epoch=4000, patch_size=24, feat_layer='relu5_4', vgg_rescale_coeff=0.006, in_memory=True, generator_path=None, fine_tuning=False):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    transform  = transforms.Compose([crop(scale, patch_size), augmentation()])\n",
    "    dataset = mydata(GT_path=GT_path, LR_path=LR_path, in_memory=in_memory, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    generator = MiniSRGAN()\n",
    "    \n",
    "    if fine_tuning:        \n",
    "        generator.load_state_dict(torch.load(generator_path))\n",
    "        print(\"pre-trained model is loaded\")\n",
    "        print(\"path : %s\"%(generator_path))\n",
    "        \n",
    "    generator = generator.to(device)\n",
    "    generator.train()\n",
    "    \n",
    "    l2_loss = nn.MSELoss()\n",
    "    g_optim = optim.Adam(generator.parameters(), lr=1e-4)\n",
    "        \n",
    "    pre_epoch = 0\n",
    "    fine_epoch = 0\n",
    "    \n",
    "    #### Train using L2_loss\n",
    "    # Initialize best loss to a very large value\n",
    "    best_loss = np.inf\n",
    "    #pre_epoch = 3526\n",
    "    #generator.load_state_dict(torch.load('./minisrgan_weights/pre_trained_model_latest.pt'))\n",
    "\n",
    "    while pre_epoch < pre_train_epoch:\n",
    "        for i, tr_data in enumerate(tqdm(loader, desc=f\"Epoch {pre_epoch+1}/{pre_train_epoch}\")):\n",
    "            gt = tr_data['GT'].to(device)\n",
    "            lr = tr_data['LR'].to(device)\n",
    "\n",
    "            output, _ = generator(lr)\n",
    "            loss = l2_loss(gt, output)\n",
    "\n",
    "            g_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            g_optim.step()\n",
    "\n",
    "        pre_epoch += 1\n",
    "\n",
    "        if pre_epoch % 2 == 0:\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/pre_trained_model_latest.pt')\n",
    "            print(pre_epoch)\n",
    "            print(loss.item())\n",
    "            print('=========')\n",
    "\n",
    "        # Save the model if it has the best loss so far\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/best_pre_trained_model.pt')\n",
    "            print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "        if pre_epoch % 800 == 0:\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/pre_trained_model_%03d.pt' % pre_epoch)\n",
    "    \n",
    "    #### Train using perceptual & adversarial loss\n",
    "    vgg_net = vgg19().to(device)\n",
    "    vgg_net = vgg_net.eval()\n",
    "    \n",
    "    discriminator = Discriminator(patch_size=patch_size * scale)\n",
    "    discriminator = discriminator.to(device)\n",
    "    discriminator.train()\n",
    "    \n",
    "    d_optim = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(g_optim, step_size=2000, gamma=0.1)\n",
    "    \n",
    "    VGG_loss = perceptual_loss(vgg_net)\n",
    "    cross_ent = nn.BCELoss()\n",
    "    tv_loss = TVLoss()\n",
    "    real_label = torch.ones((batch_size, 1)).to(device)\n",
    "    fake_label = torch.zeros((batch_size, 1)).to(device)\n",
    "    \n",
    "    generator.load_state_dict(torch.load('./minisrgan_weights/pre_trained_model_latest.pt'))\n",
    "\n",
    "    print('Training Discriminator and Generator...')\n",
    "    while fine_epoch < fine_train_epoch:\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Initialize tqdm progress bar\n",
    "        for i, tr_data in enumerate(tqdm(loader, desc=f\"Epoch {fine_epoch+1}/{fine_train_epoch}\")):\n",
    "            gt = tr_data['GT'].to(device)\n",
    "            lr = tr_data['LR'].to(device)\n",
    "\n",
    "                            \n",
    "            ## Training Discriminator\n",
    "            output, _ = generator(lr)\n",
    "            fake_prob = discriminator(output)\n",
    "            real_prob = discriminator(gt)\n",
    "            \n",
    "            d_loss_real = cross_ent(real_prob, real_label)\n",
    "            d_loss_fake = cross_ent(fake_prob, fake_label)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            g_optim.zero_grad()\n",
    "            d_optim.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "            \n",
    "            ## Training Generator\n",
    "            output, _ = generator(lr)\n",
    "            fake_prob = discriminator(output)\n",
    "            \n",
    "            _percep_loss, hr_feat, sr_feat = VGG_loss((gt + 1.0) / 2.0, (output + 1.0) / 2.0, layer=feat_layer)\n",
    "            \n",
    "            L2_loss = l2_loss(output, gt)\n",
    "            percep_loss = vgg_rescale_coeff * _percep_loss\n",
    "            adversarial_loss = adv_coeff * cross_ent(fake_prob, real_label)\n",
    "            total_variance_loss = tv_loss_coeff * tv_loss(vgg_rescale_coeff * (hr_feat - sr_feat)**2)\n",
    "            \n",
    "            g_loss = percep_loss + adversarial_loss + total_variance_loss + L2_loss\n",
    "            \n",
    "            if g_loss.item() < best_loss:\n",
    "                best_loss = g_loss.item()\n",
    "                torch.save(generator.state_dict(), './minisrgan_weights/best_trained_model.pt')\n",
    "                print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "            \n",
    "            g_optim.zero_grad()\n",
    "            d_optim.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optim.step()\n",
    "\n",
    "            \n",
    "        fine_epoch += 1\n",
    "\n",
    "        if fine_epoch % 2 == 0:\n",
    "            print(fine_epoch)\n",
    "            print(g_loss.item())\n",
    "            print(d_loss.item())\n",
    "            print('=========')\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/latest_trained_model.pt')\n",
    "\n",
    "        if fine_epoch % 500 == 0:\n",
    "            torch.save(generator.state_dict(), './minisrgan_weights/SRGAN_gene_%03d.pt' % fine_epoch)\n",
    "            torch.save(discriminator.state_dict(), './minisrgan_weights/SRGAN_discrim_%03d.pt' % fine_epoch)\n",
    "\n",
    "\n",
    "def test(LR_path, GT_path, num_workers=0, scale=4, generator_path=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = mydata(GT_path=GT_path, LR_path=LR_path, in_memory=False, transform=None)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    generator = MiniSRGAN()\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    generator = generator.to(device)\n",
    "    generator.eval()\n",
    "    \n",
    "    f = open('./result_minisrgan.txt', 'w')\n",
    "    psnr_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, te_data in enumerate(loader):\n",
    "            gt = te_data['GT'].to(device)\n",
    "            lr = te_data['LR'].to(device)\n",
    "\n",
    "            bs, c, h, w = lr.size()\n",
    "            gt = gt[:, :, : h * scale, : w * scale]\n",
    "\n",
    "            output, _ = generator(lr)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "            output = np.clip(output, -1.0, 1.0)\n",
    "            gt = gt[0].cpu().numpy()\n",
    "\n",
    "            output = (output + 1.0) / 2.0\n",
    "            gt = (gt + 1.0) / 2.0\n",
    "\n",
    "            output = output.transpose(1, 2, 0)\n",
    "            gt = gt.transpose(1, 2, 0)\n",
    "\n",
    "            y_output = rgb2ycbcr(output)[scale:-scale, scale:-scale, :1]\n",
    "            y_gt = rgb2ycbcr(gt)[scale:-scale, scale:-scale, :1]\n",
    "            \n",
    "            psnr_value = peak_signal_noise_ratio(y_output / 255.0, y_gt / 255.0, data_range=1.0)\n",
    "            psnr_list.append(psnr_value)\n",
    "            f.write(f'PSNR: {psnr_value:.4f}\\n')\n",
    "\n",
    "            result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
    "            result.save('./result_minisrgan/res_%04d.png' % i)\n",
    "\n",
    "        f.write(f'Average PSNR: {np.mean(psnr_list):.4f}')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def test_only(LR_path, num_workers=0, generator_path=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = testOnly_data(LR_path=LR_path, in_memory=False, transform=None)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    generator = MiniSRGAN()\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    generator = generator.to(device)\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, te_data in enumerate(loader):\n",
    "            lr = te_data['LR'].to(device)\n",
    "            output, _ = generator(lr)\n",
    "            output = output[0].cpu().numpy()\n",
    "            output = (output + 1.0) / 2.0\n",
    "            output = output.transpose(1, 2, 0)\n",
    "            result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
    "            result.save('./result_minisrgan/res_%04d.png' % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The original weights were trained as scripts (due to faster training). The following training is for 0 epochs to 106 epochs for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8000: 100%|██████████| 50/50 [00:15<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.0630432516336441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8000: 100%|██████████| 50/50 [00:16<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.06989946961402893\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8000: 100%|██████████| 50/50 [00:16<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.05066428706049919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8000: 100%|██████████| 50/50 [00:16<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.027863571420311928\n",
      "=========\n",
      "New best model saved with loss: 0.027863571420311928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8000: 100%|██████████| 50/50 [00:16<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.019608577713370323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8000: 100%|██████████| 50/50 [00:17<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0.03305332735180855\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8000: 100%|██████████| 50/50 [00:16<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.018714990466833115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8000: 100%|██████████| 50/50 [00:16<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0.01531557273119688\n",
      "=========\n",
      "New best model saved with loss: 0.01531557273119688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/8000: 100%|██████████| 50/50 [00:16<00:00,  3.03it/s]\n",
      "Epoch 10/8000: 100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0.021924301981925964\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/8000: 100%|██████████| 50/50 [00:18<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.013780037872493267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/8000: 100%|██████████| 50/50 [00:16<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "0.02578768879175186\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/8000: 100%|██████████| 50/50 [00:16<00:00,  3.08it/s]\n",
      "Epoch 14/8000: 100%|██████████| 50/50 [00:16<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0.016483230516314507\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/8000: 100%|██████████| 50/50 [00:16<00:00,  3.05it/s]\n",
      "Epoch 16/8000: 100%|██████████| 50/50 [00:16<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0.023890763521194458\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/8000: 100%|██████████| 50/50 [00:16<00:00,  3.03it/s]\n",
      "Epoch 18/8000: 100%|██████████| 50/50 [00:16<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "0.010968178510665894\n",
      "=========\n",
      "New best model saved with loss: 0.010968178510665894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/8000: 100%|██████████| 50/50 [00:16<00:00,  3.02it/s]\n",
      "Epoch 20/8000: 100%|██████████| 50/50 [00:16<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0.02725946716964245\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/8000: 100%|██████████| 50/50 [00:16<00:00,  3.12it/s]\n",
      "Epoch 22/8000: 100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "0.023250719532370567\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/8000: 100%|██████████| 50/50 [00:16<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.010198301635682583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/8000: 100%|██████████| 50/50 [00:16<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "0.015099535696208477\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/8000: 100%|██████████| 50/50 [00:15<00:00,  3.20it/s]\n",
      "Epoch 26/8000: 100%|██████████| 50/50 [00:19<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "0.014168597757816315\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/8000: 100%|██████████| 50/50 [00:18<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.008731229230761528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/8000: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "0.012123379856348038\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/8000: 100%|██████████| 50/50 [00:13<00:00,  3.61it/s]\n",
      "Epoch 30/8000: 100%|██████████| 50/50 [00:13<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "0.018614832311868668\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/8000: 100%|██████████| 50/50 [00:13<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with loss: 0.006930181290954351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/8000: 100%|██████████| 50/50 [00:12<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "0.01767265424132347\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/8000: 100%|██████████| 50/50 [00:12<00:00,  3.93it/s]\n",
      "Epoch 34/8000: 100%|██████████| 50/50 [00:14<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "0.019116433337330818\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/8000: 100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n",
      "Epoch 36/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0.012117142789065838\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/8000: 100%|██████████| 50/50 [00:12<00:00,  3.91it/s]\n",
      "Epoch 38/8000: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "0.012259937822818756\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Epoch 40/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "0.009144810028374195\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/8000: 100%|██████████| 50/50 [00:12<00:00,  3.95it/s]\n",
      "Epoch 42/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "0.013747246004641056\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n",
      "Epoch 44/8000: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "0.014122581109404564\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Epoch 46/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "0.008083941414952278\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n",
      "Epoch 48/8000: 100%|██████████| 50/50 [00:14<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "0.007141957525163889\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/8000: 100%|██████████| 50/50 [00:14<00:00,  3.43it/s]\n",
      "Epoch 50/8000: 100%|██████████| 50/50 [00:13<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "0.00769816292449832\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n",
      "Epoch 52/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "0.01099731121212244\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/8000: 100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n",
      "Epoch 54/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "0.010885214433073997\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n",
      "Epoch 56/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "0.014050130732357502\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Epoch 58/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "0.030997512862086296\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n",
      "Epoch 60/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "0.007158256135880947\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Epoch 62/8000: 100%|██████████| 50/50 [00:12<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "0.008513914421200752\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n",
      "Epoch 64/8000: 100%|██████████| 50/50 [00:12<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "0.016217032447457314\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/8000: 100%|██████████| 50/50 [00:12<00:00,  3.85it/s]\n",
      "Epoch 66/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "0.020708324387669563\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n",
      "Epoch 68/8000: 100%|██████████| 50/50 [00:12<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "0.01338091492652893\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n",
      "Epoch 70/8000: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "0.009677101857960224\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/8000: 100%|██████████| 50/50 [00:13<00:00,  3.78it/s]\n",
      "Epoch 72/8000: 100%|██████████| 50/50 [00:13<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "0.03464870527386665\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/8000: 100%|██████████| 50/50 [00:13<00:00,  3.78it/s]\n",
      "Epoch 74/8000: 100%|██████████| 50/50 [00:13<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "0.02053173817694187\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/8000: 100%|██████████| 50/50 [00:14<00:00,  3.41it/s]\n",
      "Epoch 76/8000: 100%|██████████| 50/50 [00:13<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "0.020234061405062675\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/8000: 100%|██████████| 50/50 [00:15<00:00,  3.33it/s]\n",
      "Epoch 78/8000: 100%|██████████| 50/50 [00:15<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "0.01577788032591343\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/8000: 100%|██████████| 50/50 [00:14<00:00,  3.53it/s]\n",
      "Epoch 80/8000: 100%|██████████| 50/50 [00:14<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "0.009388983249664307\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n",
      "Epoch 82/8000: 100%|██████████| 50/50 [00:12<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "0.014110139571130276\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/8000: 100%|██████████| 50/50 [00:12<00:00,  4.04it/s]\n",
      "Epoch 84/8000: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "0.010958055965602398\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/8000: 100%|██████████| 50/50 [00:13<00:00,  3.81it/s]\n",
      "Epoch 86/8000: 100%|██████████| 50/50 [00:12<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "0.013147730380296707\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/8000: 100%|██████████| 50/50 [00:12<00:00,  3.99it/s]\n",
      "Epoch 88/8000: 100%|██████████| 50/50 [00:14<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "0.02685728296637535\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/8000: 100%|██████████| 50/50 [00:14<00:00,  3.42it/s]\n",
      "Epoch 90/8000: 100%|██████████| 50/50 [00:13<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "0.007255207747220993\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/8000: 100%|██████████| 50/50 [00:15<00:00,  3.31it/s]\n",
      "Epoch 92/8000: 100%|██████████| 50/50 [00:14<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "0.008225744590163231\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/8000: 100%|██████████| 50/50 [00:13<00:00,  3.72it/s]\n",
      "Epoch 94/8000: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "0.030857080593705177\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/8000: 100%|██████████| 50/50 [00:14<00:00,  3.53it/s]\n",
      "Epoch 96/8000: 100%|██████████| 50/50 [00:14<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "0.017454542219638824\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/8000: 100%|██████████| 50/50 [00:14<00:00,  3.49it/s]\n",
      "Epoch 98/8000: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "0.016219528391957283\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/8000: 100%|██████████| 50/50 [00:14<00:00,  3.48it/s]\n",
      "Epoch 100/8000: 100%|██████████| 50/50 [00:14<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.010794473811984062\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/8000: 100%|██████████| 50/50 [00:12<00:00,  3.86it/s]\n",
      "Epoch 102/8000: 100%|██████████| 50/50 [00:13<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "0.006341397762298584\n",
      "=========\n",
      "New best model saved with loss: 0.006341397762298584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/8000: 100%|██████████| 50/50 [00:13<00:00,  3.73it/s]\n",
      "Epoch 104/8000: 100%|██████████| 50/50 [00:12<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "0.010456353425979614\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/8000: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s]\n",
      "Epoch 106/8000:  76%|███████▌  | 38/50 [00:12<00:05,  2.27it/s]"
     ]
    }
   ],
   "source": [
    "train(GT_path='/media/moose/Main Volume/DIV2K_Complete/DIV2K_train',LR_path='/media/moose/Main Volume/DIV2K_Complete/DIV2K_train_LR_bicubic/X4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(GT_path='../../../../Set5/image_SRF_4/HR/', LR_path='../../../../Set5/image_SRF_4/LR/', generator_path='./minisrgan_weights/best_trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
