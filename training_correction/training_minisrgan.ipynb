{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import models\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2ycbcr\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(Dataset):\n",
    "    def __init__(self, LR_path, GT_path, in_memory = True, transform = None):\n",
    "        \n",
    "        self.LR_path = LR_path\n",
    "        self.GT_path = GT_path\n",
    "        self.in_memory = in_memory\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.LR_img = sorted(os.listdir(LR_path))\n",
    "        self.GT_img = sorted(os.listdir(GT_path))\n",
    "        \n",
    "        if in_memory:\n",
    "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr)).convert(\"RGB\")).astype(np.uint8) for lr in self.LR_img]\n",
    "            self.GT_img = [np.array(Image.open(os.path.join(self.GT_path, gt)).convert(\"RGB\")).astype(np.uint8) for gt in self.GT_img]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.LR_img)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img_item = {}\n",
    "        \n",
    "        if self.in_memory:\n",
    "            GT = self.GT_img[i].astype(np.float32)\n",
    "            LR = self.LR_img[i].astype(np.float32)\n",
    "            \n",
    "        else:\n",
    "            GT = np.array(Image.open(os.path.join(self.GT_path, self.GT_img[i])).convert(\"RGB\"))\n",
    "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])).convert(\"RGB\"))\n",
    "\n",
    "        img_item['GT'] = (GT / 127.5) - 1.0\n",
    "        img_item['LR'] = (LR / 127.5) - 1.0\n",
    "                \n",
    "        if self.transform is not None:\n",
    "            img_item = self.transform(img_item)\n",
    "            \n",
    "        img_item['GT'] = img_item['GT'].transpose(2, 0, 1).astype(np.float32)\n",
    "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        return img_item\n",
    "    \n",
    "    \n",
    "class testOnly_data(Dataset):\n",
    "    def __init__(self, LR_path, in_memory = True, transform = None):\n",
    "        \n",
    "        self.LR_path = LR_path\n",
    "        self.LR_img = sorted(os.listdir(LR_path))\n",
    "        self.in_memory = in_memory\n",
    "        if in_memory:\n",
    "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr))) for lr in self.LR_img]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.LR_img)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img_item = {}\n",
    "        \n",
    "        if self.in_memory:\n",
    "            LR = self.LR_img[i]\n",
    "            \n",
    "        else:\n",
    "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])))\n",
    "\n",
    "        img_item['LR'] = (LR / 127.5) - 1.0                \n",
    "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        return img_item\n",
    "\n",
    "\n",
    "class crop(object):\n",
    "    def __init__(self, scale, patch_size):\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        LR_img, GT_img = sample['LR'], sample['GT']\n",
    "        ih, iw = LR_img.shape[:2]\n",
    "        \n",
    "        ix = random.randrange(0, iw - self.patch_size +1)\n",
    "        iy = random.randrange(0, ih - self.patch_size +1)\n",
    "        \n",
    "        tx = ix * self.scale\n",
    "        ty = iy * self.scale\n",
    "        \n",
    "        LR_patch = LR_img[iy : iy + self.patch_size, ix : ix + self.patch_size]\n",
    "        GT_patch = GT_img[ty : ty + (self.scale * self.patch_size), tx : tx + (self.scale * self.patch_size)]\n",
    "        \n",
    "        return {'LR' : LR_patch, 'GT' : GT_patch}\n",
    "\n",
    "class augmentation(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        LR_img, GT_img = sample['LR'], sample['GT']\n",
    "        \n",
    "        hor_flip = random.randrange(0,2)\n",
    "        ver_flip = random.randrange(0,2)\n",
    "        rot = random.randrange(0,2)\n",
    "    \n",
    "        if hor_flip:\n",
    "            temp_LR = np.fliplr(LR_img)\n",
    "            LR_img = temp_LR.copy()\n",
    "            temp_GT = np.fliplr(GT_img)\n",
    "            GT_img = temp_GT.copy()\n",
    "            \n",
    "            del temp_LR, temp_GT\n",
    "        \n",
    "        if ver_flip:\n",
    "            temp_LR = np.flipud(LR_img)\n",
    "            LR_img = temp_LR.copy()\n",
    "            temp_GT = np.flipud(GT_img)\n",
    "            GT_img = temp_GT.copy()\n",
    "            \n",
    "            del temp_LR, temp_GT\n",
    "            \n",
    "        if rot:\n",
    "            LR_img = LR_img.transpose(1, 0, 2)\n",
    "            GT_img = GT_img.transpose(1, 0, 2)\n",
    "        \n",
    "        \n",
    "        return {'LR' : LR_img, 'GT' : GT_img}\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 for Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg19(nn.Module):\n",
    "    \n",
    "    def __init__(self, pre_trained = True, require_grad = False):\n",
    "        super(vgg19, self).__init__()\n",
    "        self.vgg_feature = models.vgg19(pretrained = pre_trained).features\n",
    "        self.seq_list = [nn.Sequential(ele) for ele in self.vgg_feature]\n",
    "        self.vgg_layer = ['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', \n",
    "                         'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "                         'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "                         'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "                         'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5']\n",
    "        \n",
    "        if not require_grad:\n",
    "            for parameter in self.parameters():\n",
    "                parameter.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv1_1 = self.seq_list[0](x)\n",
    "        relu1_1 = self.seq_list[1](conv1_1)\n",
    "        conv1_2 = self.seq_list[2](relu1_1)\n",
    "        relu1_2 = self.seq_list[3](conv1_2)\n",
    "        pool1 = self.seq_list[4](relu1_2)\n",
    "        \n",
    "        conv2_1 = self.seq_list[5](pool1)\n",
    "        relu2_1 = self.seq_list[6](conv2_1)\n",
    "        conv2_2 = self.seq_list[7](relu2_1)\n",
    "        relu2_2 = self.seq_list[8](conv2_2)\n",
    "        pool2 = self.seq_list[9](relu2_2)\n",
    "        \n",
    "        conv3_1 = self.seq_list[10](pool2)\n",
    "        relu3_1 = self.seq_list[11](conv3_1)\n",
    "        conv3_2 = self.seq_list[12](relu3_1)\n",
    "        relu3_2 = self.seq_list[13](conv3_2)\n",
    "        conv3_3 = self.seq_list[14](relu3_2)\n",
    "        relu3_3 = self.seq_list[15](conv3_3)\n",
    "        conv3_4 = self.seq_list[16](relu3_3)\n",
    "        relu3_4 = self.seq_list[17](conv3_4)\n",
    "        pool3 = self.seq_list[18](relu3_4)\n",
    "        \n",
    "        conv4_1 = self.seq_list[19](pool3)\n",
    "        relu4_1 = self.seq_list[20](conv4_1)\n",
    "        conv4_2 = self.seq_list[21](relu4_1)\n",
    "        relu4_2 = self.seq_list[22](conv4_2)\n",
    "        conv4_3 = self.seq_list[23](relu4_2)\n",
    "        relu4_3 = self.seq_list[24](conv4_3)\n",
    "        conv4_4 = self.seq_list[25](relu4_3)\n",
    "        relu4_4 = self.seq_list[26](conv4_4)\n",
    "        pool4 = self.seq_list[27](relu4_4)\n",
    "        \n",
    "        conv5_1 = self.seq_list[28](pool4)\n",
    "        relu5_1 = self.seq_list[29](conv5_1)\n",
    "        conv5_2 = self.seq_list[30](relu5_1)\n",
    "        relu5_2 = self.seq_list[31](conv5_2)\n",
    "        conv5_3 = self.seq_list[32](relu5_2)\n",
    "        relu5_3 = self.seq_list[33](conv5_3)\n",
    "        conv5_4 = self.seq_list[34](relu5_3)\n",
    "        relu5_4 = self.seq_list[35](conv5_4)\n",
    "        pool5 = self.seq_list[36](relu5_4)\n",
    "        \n",
    "        vgg_output = namedtuple(\"vgg_output\", self.vgg_layer)\n",
    "        \n",
    "        vgg_list = [conv1_1, relu1_1, conv1_2, relu1_2, pool1, \n",
    "                         conv2_1, relu2_1, conv2_2, relu2_2, pool2,\n",
    "                         conv3_1, relu3_1, conv3_2, relu3_2, conv3_3, relu3_3, conv3_4, relu3_4, pool3,\n",
    "                         conv4_1, relu4_1, conv4_2, relu4_2, conv4_3, relu4_3, conv4_4, relu4_4, pool4,\n",
    "                         conv5_1, relu5_1, conv5_2, relu5_2, conv5_3, relu5_3, conv5_4, relu5_4, pool5]\n",
    "        \n",
    "        out = vgg_output(*vgg_list)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, rgb_range = 1,\n",
    "        norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), sign=-1):\n",
    "\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(norm_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(norm_mean) / std\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "class perceptual_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, vgg):\n",
    "        super(perceptual_loss, self).__init__()\n",
    "        self.normalization_mean = [0.485, 0.456, 0.406]\n",
    "        self.normalization_std = [0.229, 0.224, 0.225]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.transform = MeanShift(norm_mean = self.normalization_mean, norm_std = self.normalization_std).to(self.device)\n",
    "        self.vgg = vgg\n",
    "        self.criterion = nn.MSELoss()\n",
    "    def forward(self, HR, SR, layer = 'relu5_4'):\n",
    "        ## HR and SR should be normalized [0,1]\n",
    "        hr = self.transform(HR)\n",
    "        sr = self.transform(SR)\n",
    "        \n",
    "        hr_feat = getattr(self.vgg(hr), layer)\n",
    "        sr_feat = getattr(self.vgg(sr), layer)\n",
    "        \n",
    "        return self.criterion(hr_feat, sr_feat), hr_feat, sr_feat\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        \n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias):\n",
    "        super(_conv, self).__init__(in_channels = in_channels, out_channels = out_channels, \n",
    "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True)\n",
    "        \n",
    "        self.weight.data = torch.normal(torch.zeros((out_channels, in_channels, kernel_size, kernel_size)), 0.02)\n",
    "        self.bias.data = torch.zeros((out_channels))\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, BN = False, act = None, stride = 1, bias = True):\n",
    "        super(conv, self).__init__()\n",
    "        m = []\n",
    "        m.append(_conv(in_channels = in_channel, out_channels = out_channel, \n",
    "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True))\n",
    "        \n",
    "        if BN:\n",
    "            m.append(nn.BatchNorm2d(num_features = out_channel))\n",
    "        \n",
    "        if act is not None:\n",
    "            m.append(act)\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, act = nn.ReLU(inplace = True), bias = True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(channels, channels, kernel_size, BN = True, act = act))\n",
    "        m.append(conv(channels, channels, kernel_size, BN = True, act = None))\n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, num_res_block, act = nn.ReLU(inplace = True)):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        m = []\n",
    "        \n",
    "        self.conv = conv(in_channels, out_channels, kernel_size, BN = False, act = act)\n",
    "        for i in range(num_res_block):\n",
    "            m.append(ResBlock(out_channels, kernel_size, act))\n",
    "        \n",
    "        m.append(conv(out_channels, out_channels, kernel_size, BN = True, act = None))\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.conv(x)\n",
    "        out = self.body(res)\n",
    "        out += res\n",
    "        \n",
    "        return out\n",
    "        \n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, channel, kernel_size, scale, act = nn.ReLU(inplace = True)):\n",
    "        super(Upsampler, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(channel, channel * scale * scale, kernel_size))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "    \n",
    "        if act is not None:\n",
    "            m.append(act)\n",
    "        \n",
    "        self.body = nn.Sequential(*m)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n",
    "\n",
    "class discrim_block(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, kernel_size, act = nn.LeakyReLU(inplace = True)):\n",
    "        super(discrim_block, self).__init__()\n",
    "        m = []\n",
    "        m.append(conv(in_feats, out_feats, kernel_size, BN = True, act = act))\n",
    "        m.append(conv(out_feats, out_feats, kernel_size, BN = True, act = act, stride = 2))\n",
    "        self.body = nn.Sequential(*m)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileSR(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_feat = 3, n_feats = 32, kernel_size = 3, num_block = 6, act = nn.PReLU(), scale=4):\n",
    "        super(MobileSR, self).__init__()\n",
    "        \n",
    "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 9, BN = False, act = act)\n",
    "        \n",
    "        resblocks = [ResBlock(channels = n_feats, kernel_size = 3, act = act) for _ in range(num_block)]\n",
    "        self.body = nn.Sequential(*resblocks)\n",
    "        \n",
    "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = True, act = None)\n",
    "        \n",
    "        if(scale == 4):\n",
    "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = 2, act = act) for _ in range(2)]\n",
    "        else:\n",
    "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = scale, act = act)]\n",
    "\n",
    "        self.tail = nn.Sequential(*upsample_blocks)\n",
    "        \n",
    "        self.last_conv = conv(in_channel = n_feats, out_channel = img_feat, kernel_size = 3, BN = False, act = nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv01(x)\n",
    "        _skip_connection = x\n",
    "        \n",
    "        x = self.body(x)\n",
    "        x = self.conv02(x)\n",
    "        feat = x + _skip_connection\n",
    "        \n",
    "        x = self.tail(feat)\n",
    "        x = self.last_conv(x)\n",
    "        \n",
    "        return x, feat\n",
    "     \n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_feat = 3, n_feats = 64, kernel_size = 3, act = nn.LeakyReLU(inplace = True), num_of_block = 3, patch_size = 96):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.act = act\n",
    "        \n",
    "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act)\n",
    "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act, stride = 2)\n",
    "        \n",
    "        body = [discrim_block(in_feats = n_feats * (2 ** i), out_feats = n_feats * (2 ** (i + 1)), kernel_size = 3, act = self.act) for i in range(num_of_block)]    \n",
    "        self.body = nn.Sequential(*body)\n",
    "        \n",
    "        self.linear_size = ((patch_size // (2 ** (num_of_block + 1))) ** 2) * (n_feats * (2 ** num_of_block))\n",
    "        \n",
    "        tail = []\n",
    "        \n",
    "        tail.append(nn.Linear(self.linear_size, 1024))\n",
    "        tail.append(self.act)\n",
    "        tail.append(nn.Linear(1024, 1))\n",
    "        tail.append(nn.Sigmoid())\n",
    "        \n",
    "        self.tail = nn.Sequential(*tail)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv01(x)\n",
    "        x = self.conv02(x)\n",
    "        x = self.body(x)        \n",
    "        x = x.view(-1, self.linear_size)\n",
    "        x = self.tail(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(LR_path, GT_path, batch_size=16, res_num=16, num_workers=0, scale=4, L2_coeff=1.0, adv_coeff=1e-3, tv_loss_coeff=0.0, pre_train_epoch=8000, fine_train_epoch=4000, patch_size=24, feat_layer='relu5_4', vgg_rescale_coeff=0.006, in_memory=True, generator_path=None, fine_tuning=False):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    transform  = transforms.Compose([crop(scale, patch_size), augmentation()])\n",
    "    dataset = mydata(GT_path=GT_path, LR_path=LR_path, in_memory=in_memory, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    generator = MobileSR()\n",
    "    \n",
    "    if fine_tuning:        \n",
    "        generator.load_state_dict(torch.load(generator_path))\n",
    "        print(\"pre-trained model is loaded\")\n",
    "        print(\"path : %s\"%(generator_path))\n",
    "        \n",
    "    generator = generator.to(device)\n",
    "    generator.train()\n",
    "    \n",
    "    l2_loss = nn.MSELoss()\n",
    "    g_optim = optim.Adam(generator.parameters(), lr=1e-4)\n",
    "        \n",
    "    pre_epoch = 0\n",
    "    fine_epoch = 0\n",
    "    \n",
    "    #### Train using L2_loss\n",
    "    # Initialize best loss to a very large value\n",
    "    best_loss = 0.002\n",
    "\n",
    "    pre_epoch = 0\n",
    "\n",
    "    while pre_epoch < pre_train_epoch:\n",
    "        for i, tr_data in enumerate(tqdm(loader, desc=f\"Epoch {pre_epoch+1}/{pre_train_epoch}\")):\n",
    "            gt = tr_data['GT'].to(device)\n",
    "            lr = tr_data['LR'].to(device)\n",
    "\n",
    "            output, _ = generator(lr)\n",
    "            loss = l2_loss(gt, output)\n",
    "\n",
    "            g_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            g_optim.step()\n",
    "\n",
    "        pre_epoch += 1\n",
    "\n",
    "        if pre_epoch % 2 == 0:\n",
    "            torch.save(generator.state_dict(), './mobilesr_weights/pre_trained_model_latest.pt')\n",
    "            print(pre_epoch)\n",
    "            print(loss.item())\n",
    "            print('=========')\n",
    "\n",
    "        # Save the model if it has the best loss so far\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(generator.state_dict(), './mobilesr_weights/best_pre_trained_model.pt')\n",
    "            print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "        if pre_epoch % 800 == 0:\n",
    "            torch.save(generator.state_dict(), './mobilesr_weights/pre_trained_model_%03d.pt' % pre_epoch)\n",
    "    \n",
    "    #### Train using perceptual & adversarial loss\n",
    "    vgg_net = vgg19().to(device)\n",
    "    vgg_net = vgg_net.eval()\n",
    "    \n",
    "    discriminator = Discriminator(patch_size=patch_size * scale)\n",
    "    discriminator = discriminator.to(device)\n",
    "    discriminator.train()\n",
    "    \n",
    "    d_optim = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(g_optim, step_size=2000, gamma=0.1)\n",
    "    \n",
    "    VGG_loss = perceptual_loss(vgg_net)\n",
    "    cross_ent = nn.BCELoss()\n",
    "    tv_loss = TVLoss()\n",
    "    real_label = torch.ones((batch_size, 1)).to(device)\n",
    "    fake_label = torch.zeros((batch_size, 1)).to(device)\n",
    "    \n",
    "    generator.load_state_dict(torch.load('./mobilesr_weights/pre_trained_model_latest.pt'))\n",
    "\n",
    "    print('Training Discriminator and Generator...')\n",
    "    while fine_epoch < fine_train_epoch:\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Initialize tqdm progress bar\n",
    "        for i, tr_data in enumerate(tqdm(loader, desc=f\"Epoch {fine_epoch+1}/{fine_train_epoch}\")):\n",
    "            gt = tr_data['GT'].to(device)\n",
    "            lr = tr_data['LR'].to(device)\n",
    "\n",
    "                            \n",
    "            ## Training Discriminator\n",
    "            output, _ = generator(lr)\n",
    "            fake_prob = discriminator(output)\n",
    "            real_prob = discriminator(gt)\n",
    "            \n",
    "            d_loss_real = cross_ent(real_prob, real_label)\n",
    "            d_loss_fake = cross_ent(fake_prob, fake_label)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            g_optim.zero_grad()\n",
    "            d_optim.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "            \n",
    "            ## Training Generator\n",
    "            output, _ = generator(lr)\n",
    "            fake_prob = discriminator(output)\n",
    "            \n",
    "            _percep_loss, hr_feat, sr_feat = VGG_loss((gt + 1.0) / 2.0, (output + 1.0) / 2.0, layer=feat_layer)\n",
    "            \n",
    "            L2_loss = l2_loss(output, gt)\n",
    "            percep_loss = vgg_rescale_coeff * _percep_loss\n",
    "            adversarial_loss = adv_coeff * cross_ent(fake_prob, real_label)\n",
    "            total_variance_loss = tv_loss_coeff * tv_loss(vgg_rescale_coeff * (hr_feat - sr_feat)**2)\n",
    "            \n",
    "            g_loss = percep_loss + adversarial_loss + total_variance_loss + L2_loss\n",
    "            \n",
    "            if g_loss.item() < best_loss:\n",
    "                best_loss = g_loss.item()\n",
    "                torch.save(generator.state_dict(), './mobilesr_weights/best_trained_model.pt')\n",
    "                print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "            \n",
    "            g_optim.zero_grad()\n",
    "            d_optim.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optim.step()\n",
    "\n",
    "            \n",
    "        fine_epoch += 1\n",
    "\n",
    "        if fine_epoch % 2 == 0:\n",
    "            print(fine_epoch)\n",
    "            print(g_loss.item())\n",
    "            print(d_loss.item())\n",
    "            print('=========')\n",
    "            torch.save(generator.state_dict(), './mobilesr_weights/latest_trained_model.pt')\n",
    "\n",
    "        if fine_epoch % 500 == 0:\n",
    "            torch.save(generator.state_dict(), './mobilesr_weights/SRGAN_gene_%03d.pt' % fine_epoch)\n",
    "            torch.save(discriminator.state_dict(), './mobilesr_weights/SRGAN_discrim_%03d.pt' % fine_epoch)\n",
    "\n",
    "\n",
    "def test(LR_path, GT_path, num_workers=0, scale=4, generator_path=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = mydata(GT_path=GT_path, LR_path=LR_path, in_memory=False, transform=None)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    generator = MobileSR()\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    generator = generator.to(device)\n",
    "    generator.eval()\n",
    "    \n",
    "    f = open('./result_mobilesr.txt', 'w')\n",
    "    psnr_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, te_data in enumerate(loader):\n",
    "            gt = te_data['GT'].to(device)\n",
    "            lr = te_data['LR'].to(device)\n",
    "\n",
    "            bs, c, h, w = lr.size()\n",
    "            gt = gt[:, :, : h * scale, : w * scale]\n",
    "\n",
    "            output, _ = generator(lr)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "            output = np.clip(output, -1.0, 1.0)\n",
    "            gt = gt[0].cpu().numpy()\n",
    "\n",
    "            output = (output + 1.0) / 2.0\n",
    "            gt = (gt + 1.0) / 2.0\n",
    "\n",
    "            output = output.transpose(1, 2, 0)\n",
    "            gt = gt.transpose(1, 2, 0)\n",
    "\n",
    "            y_output = rgb2ycbcr(output)[scale:-scale, scale:-scale, :1]\n",
    "            y_gt = rgb2ycbcr(gt)[scale:-scale, scale:-scale, :1]\n",
    "            \n",
    "            psnr_value = peak_signal_noise_ratio(y_output / 255.0, y_gt / 255.0, data_range=1.0)\n",
    "            psnr_list.append(psnr_value)\n",
    "            f.write(f'PSNR: {psnr_value:.4f}\\n')\n",
    "\n",
    "            result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
    "            result.save('./result_mobilesr/res_%04d.png' % i)\n",
    "\n",
    "        f.write(f'Average PSNR: {np.mean(psnr_list):.4f}')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def test_only(LR_path, num_workers=0, generator_path=None):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = testOnly_data(LR_path=LR_path, in_memory=False, transform=None)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    generator = MobileSR()\n",
    "    generator.load_state_dict(torch.load(generator_path))\n",
    "    generator = generator.to(device)\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, te_data in enumerate(loader):\n",
    "            lr = te_data['LR'].to(device)\n",
    "            output, _ = generator(lr)\n",
    "            output = output[0].cpu().numpy()\n",
    "            output = (output + 1.0) / 2.0\n",
    "            output = output.transpose(1, 2, 0)\n",
    "            result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
    "            result.save('./result_mobilesr/res_%04d.png' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8000: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
      "Epoch 2/8000: 100%|██████████| 50/50 [00:16<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.08938774466514587\n",
      "=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8000: 100%|██████████| 50/50 [00:14<00:00,  3.42it/s]\n",
      "Epoch 4/8000:  22%|██▏       | 11/50 [00:03<00:11,  3.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGT_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Dataset/DIV2K_train_HR/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mLR_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Dataset/DIV2K_train_LR_bicubic_X4/DIV2K_train_LR_bicubic/X4/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(LR_path, GT_path, batch_size, res_num, num_workers, scale, L2_coeff, adv_coeff, tv_loss_coeff, pre_train_epoch, fine_train_epoch, patch_size, feat_layer, vgg_rescale_coeff, in_memory, generator_path, fine_tuning)\u001b[0m\n\u001b[1;32m     29\u001b[0m pre_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pre_epoch \u001b[38;5;241m<\u001b[39m pre_train_epoch:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tr_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_train_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m     33\u001b[0m         gt \u001b[38;5;241m=\u001b[39m tr_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m         lr \u001b[38;5;241m=\u001b[39m tr_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-rocm/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-rocm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-rocm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-rocm/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-rocm/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mmydata.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     22\u001b[0m img_item \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_memory:\n\u001b[0;32m---> 25\u001b[0m     GT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGT_img\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLR_img[i]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(GT_path='../../Dataset/DIV2K_train_HR/',LR_path='../../Dataset/DIV2K_train_LR_bicubic_X4/DIV2K_train_LR_bicubic/X4/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
